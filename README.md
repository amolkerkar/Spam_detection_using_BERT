## BERT - Bidirectional Encoder Representations from Transformers
It stands out as the first NLP technique that exclusively relies on a self-attention mechanism, made possible by the bidirectional Transformers at the core of its design. This is noteworthy because the meaning of a word can often change as a sentence unfolds. With each added word, the overall significance of the focused word is enhanced by the NLP algorithm. The presence of more words in a sentence or phrase increases the ambiguity of the focus word. BERT addresses this by reading bidirectionally, taking into account the impact of all other words in a sentence on the focus word. It eliminates the left-to-right bias that words may develop towards a specific meaning as the sentence progresses.
In this project is a BERT implementation on spamdata dataset.
